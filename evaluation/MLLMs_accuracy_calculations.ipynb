{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2bb1400-1059-4cce-a7a7-fb6ffff898e5",
   "metadata": {},
   "source": [
    "### Once you ran evaluate_MLLMs.py for all models (or the ones you desire),\n",
    "### you can modify the code below to combine all of them and calculate accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50f91b3-a0ec-4578-8e04-25e92a516018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#MODIFY THIS BASED ON THE TASKS YOU WANT TO EVALUATE\n",
    "base_dfs = {\"shape_id\":None, \"sides_id\": None, \"two_shapes\": None, \"abstract\": None}  \n",
    "\n",
    "#MODIFY THIS BASED ON THE MODELS YOU WANT TO EVALUATE\n",
    "model_columns = {\n",
    "    \"llava-1.5\": \"generate_text_llava_1.5\",\n",
    "    \"llava-1.6\": \"generate_text_llava_1.6\",\n",
    "    \"qwen\": \"generate_text_qwen\",\n",
    "    \"internvl\": \"generate_text_internvl\",\n",
    "    \"llava-one\": \"generate_text_llava-one\",\n",
    "    \"llama-3.2\": \"generate_text_llama-3.2\",\n",
    "    \"gpt-4-turbo\": \"generate_text_gpt-4-turbo\",\n",
    "    \"gpt-4o\": \"generate_text_gpt-4o\",\n",
    "    \"janus\": \"generate_text_janus\",\n",
    "    \"molmo\": \"generate_text_molmo\",\n",
    "    \"math-llava\": \"generate_text_math-llava\",\n",
    "    \"g-llava\": \"generate_text_g-llava\",\n",
    "    \"math-puma\": \"generate_text_math-puma\",\n",
    "    \n",
    "}\n",
    "additional_columns = {task: {col: None for col in model_columns.values()} for task in base_dfs.keys()}\n",
    "\n",
    "# Iterate over files in the current directory\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\".csv\") and \"text_only\" not in file: \n",
    "        for task in base_dfs.keys(): \n",
    "            if task in file:\n",
    "                for model_version, column_name in model_columns.items():\n",
    "                    if f\"{model_version}_\" in file:\n",
    "                        print(f\"Processing {file} for {task}...\")\n",
    "                        df = pd.read_csv(file)\n",
    "                        print(len(df))\n",
    "                        \n",
    "                        # For the first model (base_df), take all columns\n",
    "                        if base_dfs[task] is None and model_version == \"llava-1.5\":\n",
    "                            base_dfs[task] = df.copy()\n",
    "                            base_dfs[task].rename(columns={\"generated_text\": column_name}, inplace=True)\n",
    "                        else:\n",
    "                            # Just take the generated_text column and rename it\n",
    "                            additional_columns[task][column_name] = df[\"generated_text\"]\n",
    "\n",
    "# Append the additional columns to the base DataFrames\n",
    "for task, base_df in base_dfs.items():\n",
    "    if base_df is not None:\n",
    "        for column_name, column_data in additional_columns[task].items():\n",
    "            if column_data is not None:\n",
    "                base_df[column_name] = column_data\n",
    "\n",
    "        # Save the final combined DataFrame for each task\n",
    "        output_file = f\"final_combined_{task}_generated_texts.csv\"\n",
    "        base_df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved combined DataFrame for {task} to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90f13fa-dde0-4542-86d7-10c7c1c8273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file names for the combined outputs\n",
    "two_shapes_file = \"final_combined_two_shapes_generated_texts.csv\"\n",
    "shape_id_file = \"final_combined_shape_id_generated_texts.csv\"\n",
    "sides_id_file = \"final_combined_sides_id_generated_texts.csv\"\n",
    "\n",
    "# Load the DataFrames\n",
    "two_shapes_df = pd.read_csv(two_shapes_file)\n",
    "shape_id_df = pd.read_csv(shape_id_file)\n",
    "sides_id_df = pd.read_csv(sides_id_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b82e9ab-dcb2-4f32-a9e4-d666831a4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### shape_id ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b03e096-d142-4680-bf37-fe64eae325e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_replace = [\"diamond\", \"rhombus\", \"quadrilateral\"]\n",
    "replacement_word = \"square\"\n",
    "\n",
    "# Process only columns that start with 'generate_text_'\n",
    "for col in shape_id_df.columns:\n",
    "    if col.startswith(\"generate_text_\"):\n",
    "        shape_id_df[col] = (\n",
    "            shape_id_df[col]\n",
    "            .astype(str)  # Convert all values to string\n",
    "            .str.lower()   # Lowercase the text\n",
    "            .replace({word: replacement_word for word in words_to_replace}, regex=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fcaf01-bc21-4726-be93-2273380995d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Define a function to calculate inclusion accuracy\n",
    "def is_inclusive_match(true_shape, predicted_shape):\n",
    "    \"\"\"\n",
    "    Returns True if the predicted shape contains the true shape as a substring\n",
    "    or has a high similarity score.\n",
    "    \"\"\"\n",
    "    if pd.isna(predicted_shape):  # Handle NaN cases\n",
    "        return False\n",
    "        \n",
    "    predicted_shape = predicted_shape.strip().lower()\n",
    "    true_shape = true_shape.strip().lower()\n",
    "    \n",
    "    if true_shape in predicted_shape:\n",
    "        return True\n",
    "        \n",
    "    # Exclude specific sentence from inclusion accuracy\n",
    "    if predicted_shape == \"the image contains a variety of shapes, including circles, squares, and triangles.\":\n",
    "        return False\n",
    "        \n",
    "    return False\n",
    "\n",
    "# Models to evaluate\n",
    "models = ['generate_text_llava_1.5', 'generate_text_llava_1.6',\n",
    "          'generate_text_qwen', 'generate_text_internvl', 'generate_text_llava-one',\n",
    "          'generate_text_llama-3.2', 'generate_text_gpt-4-turbo', 'generate_text_gpt-4o',\n",
    "          'generate_text_molmo', 'generate_text_janus',\n",
    "          \"generate_text_math-llava\",\"generate_text_g-llava\", \"generate_text_math-puma\" ]\n",
    "\n",
    "# Create correctness, inclusion correctness, and extended accuracy columns for each model\n",
    "for model in models:\n",
    "    shape_id_df[f'{model}_is_correct'] = shape_id_df['shape'] == shape_id_df[model].str.strip().str.lower()\n",
    "    shape_id_df[f'{model}_is_inclusive'] = shape_id_df.apply(\n",
    "        lambda row: is_inclusive_match(row['shape'], row[model]), axis=1\n",
    "    )\n",
    "\n",
    "# Initialize dictionaries to hold per-model accuracies\n",
    "model_correctness = {}\n",
    "\n",
    "# Group by shape and calculate exact, inclusion, and extended accuracies for each model\n",
    "for model in models:\n",
    "    # Exact accuracy\n",
    "    model_accuracy = shape_id_df.groupby('shape').apply(lambda group: {\n",
    "        'Exact Accuracy': group[f'{model}_is_correct'].mean(),\n",
    "        'Inclusion Accuracy': group[f'{model}_is_inclusive'].mean(),\n",
    "    }).apply(pd.Series)\n",
    "    \n",
    "    # Reset index and store the result in the dictionary\n",
    "    model_correctness[model] = model_accuracy.reset_index()\n",
    "\n",
    "# Display the correctness and inclusion accuracy per model\n",
    "for model, correctness_df in model_correctness.items():\n",
    "    print(f\"\\nAccuracy for {model}:\")\n",
    "    print(correctness_df[['shape','Inclusion Accuracy']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e4622-6da8-43be-b737-49ac4898bb8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf11a0-036d-4b65-bf46-98157c60950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### sides_id ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6fa64-918a-4c38-85eb-59c0a1cf2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sides_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9475b3a2-4ab0-4ac7-b3a3-1cb86093785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_number(text):\n",
    "    words_to_numbers = {\n",
    "        \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\", \"five\": \"5\", \"six\": \"6\",\n",
    "        \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\", \"ten\": \"10\"\n",
    "    }\n",
    "\n",
    "    # Replace word numbers with digits\n",
    "    for word, number in words_to_numbers.items():\n",
    "        text = re.sub(rf'\\b{word}\\b', number, text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Extract the first numeric value in the string\n",
    "    match = re.search(r'\\b\\d+\\b', text)\n",
    "    return match.group(0) if match else \"\"  # Return extracted number or empty string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b6169-096c-4040-ba38-2db1f2d27a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply extract_number to all columns starting with \"generate_text\"\n",
    "sides_id_df.loc[:, sides_id_df.columns.str.startswith(\"generate_text\")] = (\n",
    "    sides_id_df.filter(like=\"generate_text\").apply(extract_number)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f137abc0-07d2-47da-b3a4-cc6bd5169472",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['generate_text_llava_1.5', 'generate_text_llava_1.6',\n",
    "          'generate_text_qwen', 'generate_text_internvl', 'generate_text_llava-one',\n",
    "          'generate_text_llama-3.2', 'generate_text_gpt-4-turbo', 'generate_text_gpt-4o',\n",
    "          'generate_text_molmo', 'generate_text_janus',\n",
    "          \"generate_text_math-llava\",\"generate_text_g-llava\", \"generate_text_math-puma\" ]\n",
    "\n",
    "#'generate_text_llava-one',\n",
    "shape_to_sides = {\n",
    "    \"triangle\": 3,\n",
    "    \"square\": 4,\n",
    "    \"pentagon\": 5,\n",
    "    \"hexagon\": 6,\n",
    "    \"heptagon\": 7,\n",
    "    \"octagon\": 8\n",
    "}\n",
    "\n",
    "# Create a new column \"sides\" based on the \"shape\" column\n",
    "sides_id_df[\"sides\"] = sides_id_df[\"shape\"].str.lower().map(shape_to_sides)\n",
    "\n",
    "# Function to compute accuracy between \"sides\" and model predictions\n",
    "def compute_sides_accuracy(df, model_columns, sides_column):\n",
    "    accuracies = {}\n",
    "\n",
    "    for model in model_columns:\n",
    "        # Convert predictions to integer (handling NaNs)\n",
    "        predictions = pd.to_numeric(df[model], errors='coerce')\n",
    "        \n",
    "        # Compare with ground truth\n",
    "        correct_predictions = predictions == df[sides_column]\n",
    "        \n",
    "        # Compute accuracy\n",
    "        accuracies[model] = correct_predictions.mean()\n",
    "    \n",
    "    return pd.DataFrame.from_dict(accuracies, orient='index', columns=['Accuracy'])\n",
    "\n",
    "# Compute accuracy for each model\n",
    "sides_accuracy_df = compute_sides_accuracy(sides_id_df, models, \"sides\")\n",
    "sides_accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de9d22-2198-41a8-8a84-79a21287bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_shape_accuracy(df, model_columns, shape_column, sides_column):\n",
    "    accuracies = {shape: {} for shape in df[shape_column].unique()}\n",
    "\n",
    "    for model in model_columns:\n",
    "        # Convert predictions to integer (handling NaNs)\n",
    "        predictions = pd.to_numeric(df[model], errors='coerce')\n",
    "\n",
    "        # Compute accuracy for each shape\n",
    "        for shape in df[shape_column].unique():\n",
    "            shape_df = df[df[shape_column] == shape]\n",
    "            correct_predictions = (predictions.loc[shape_df.index] == shape_df[sides_column]).mean()\n",
    "            accuracies[shape][model] = correct_predictions\n",
    "\n",
    "    # Convert dictionary to DataFrame with models as rows and shapes as columns\n",
    "    return pd.DataFrame(accuracies) #.T\n",
    "\n",
    "\n",
    "# Compute accuracy per shape per model\n",
    "per_shape_accuracy_df = compute_per_shape_accuracy(sides_id_df, models, \"shape\", \"sides\")\n",
    "\n",
    "per_shape_accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea470b-dde2-4931-b350-6204dc6846a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68154e76-b560-480c-9482-b8568ddd1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### two_shapes ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec559ba-88ff-4c9f-809f-03ade61a9e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file names for the combined outputs\n",
    "two_shapes_file = \"final_combined_two_shapes_generated_texts.csv\"\n",
    "two_shapes_df = pd.read_csv(two_shapes_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01c32a-a89e-4cae-8dae-313d4fe2d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_process = ['generate_text_llava_1.5', 'generate_text_llava_1.6', 'generate_text_qwen',\n",
    "                      'generate_text_internvl', 'generate_text_llava-one', 'generate_text_llama-3.2',\n",
    "                      'generate_text_gpt-4-turbo', 'generate_text_gpt-4o', 'generate_text_janus', 'generate_text_molmo',\n",
    "                     'generate_text_math-puma', 'generate_text_math-llava', 'generate_text_g-llava']\n",
    "\n",
    "# Replace \"diamond\", \"quadrilateral\", and \"rhombus\" with \"square\" in the specified columns\n",
    "for column in columns_to_process:\n",
    "    two_shapes_df[column] = two_shapes_df[column].str.replace(\n",
    "        r'\\b(diamond|quadrilateral|rhombus)\\b', 'square', case=False, regex=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b261d-03d0-42dd-8089-bcea55b81b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_shapes_df[\"generate_text_janus\"] = two_shapes_df[\"generate_text_janus\"].str.split(\"\\n\\n### Final Answer:\").str[0]\n",
    "two_shapes_df[\"generate_text_janus\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd1c5f-ce6e-4ca0-8ce4-7a01d2a609c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Refine function to clean up redundant line breaks\n",
    "def combine_steps_final_cleaned(text):\n",
    "    # Step 1: Check if \"4.\" exists and is not at the end of the text\n",
    "    if \"\\n4.\" in text and not text.endswith(\"4.\"):\n",
    "        # Split into parts and find the section with \"3.\"\n",
    "        parts = text.split(\"\\n3.\")\n",
    "\n",
    "        # Ensure there's content to combine\n",
    "        # Combine the content of \"2.\" and \"3.\"\n",
    "        combined_part = parts[0].strip() + \" \" + parts[1].strip()\n",
    "        # Combine everything back, removing unnecessary \"3.\" and renumbering \"4.\" to \"3.\"\n",
    "        final_text = combined_part.replace(\"\\n4.\", \"\\n3.\")\n",
    "\n",
    "        # Remove redundant line breaks\n",
    "        #final_text = re.sub(r'\\n+', '\\n', final_text)\n",
    "        return final_text.strip()\n",
    "    \n",
    "    # If \"4.\" is at the end or not present, return the text as is\n",
    "    return re.sub(r'\\n+', '\\n', text.strip())\n",
    "\n",
    "# Apply the refined function to the DataFrame\n",
    "two_shapes_df['generate_text_qwen'] = two_shapes_df['generate_text_qwen'].apply(combine_steps_final_cleaned)\n",
    "\n",
    "# Display the updated text\n",
    "two_shapes_df['generate_text_qwen'].iloc[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524be426-6def-400d-b2d5-7d43c871ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_shapes_df['generate_text_janus'] = two_shapes_df['generate_text_janus'].apply(combine_steps_final_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b735f684-4ce0-4181-bed4-1d017eeb36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_shapes_df['ground_truth_shapes'] = two_shapes_df.apply(lambda row: [row['shape1'], row['shape2']], axis=1)\n",
    "two_shapes_df['ground_truth_shapes'] = two_shapes_df['ground_truth_shapes'].apply(sorted)\n",
    "\n",
    "# Step 2: Create a dictionary for the number of sides each shape has\n",
    "shape_sides_dict = {\n",
    "    'triangle': 3,\n",
    "    'square': 4,\n",
    "    'rectangle': 4,\n",
    "    'circle': 0,  # Assuming circles have no sides\n",
    "    'ellipse': 0,  # Assuming ellipses have no sides\n",
    "    'pentagon': 5,\n",
    "    'hexagon': 6\n",
    "}\n",
    "\n",
    "# Step 3: Count the total number of sides based on the ground truth shapes\n",
    "def calculate_total_sides(shapes, sides_dict):\n",
    "    return sum(sides_dict[shape] for shape in shapes if shape in sides_dict)\n",
    "\n",
    "two_shapes_df['total_ground_truth_sides'] = two_shapes_df['ground_truth_shapes'].apply(\n",
    "    lambda shapes: calculate_total_sides(shapes, shape_sides_dict)\n",
    ")\n",
    "\n",
    "# Display the updated dataframe with relevant columns\n",
    "two_shapes_df[['ground_truth_shapes', 'total_ground_truth_sides']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48524b88-a3f7-41d0-9935-3f5fdd9cfc4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Define common shapes\n",
    "common_shapes = [\"triangle\", \"square\", \"rectangle\", \"circle\", \"ellipse\", \"pentagon\", \"hexagon\", \"octagon\", \"diamond\", \"heptagon\", \"cube\"]\n",
    "\n",
    "# Function to extract and count shapes accurately\n",
    "def extract_shapes(text):\n",
    "    # Extract text between \"1.\" and \"2.\"\n",
    "    #print(text)\n",
    "    match = re.search(r\"1\\.\\s(.*?)\\s2\\.\", text, re.DOTALL)\n",
    "    if match:\n",
    "        shape_text = match.group(1).lower()  # Extract and lowercase text\n",
    "        # Match each shape in the text, allowing for multiple occurrences\n",
    "        shape_counts = Counter()\n",
    "        for shape in common_shapes:\n",
    "            occurrences = re.findall(fr'\\b{shape}\\b', shape_text)  # Count exact shape matches\n",
    "            shape_counts[shape] += len(occurrences)\n",
    "        # Remove shapes with zero occurrences\n",
    "        return {k: v for k, v in shape_counts.items() if v > 0}\n",
    "    return {}\n",
    "\n",
    "# Function to convert ground truth shapes to a counter\n",
    "def ground_truth_to_counter(shape_list):\n",
    "    return dict(Counter(shape_list))\n",
    "\n",
    "# Function to compare shape counts and ground truth counts\n",
    "def compare_counts(row):\n",
    "    ground_truth_counter = ground_truth_to_counter(row['ground_truth_shapes'])\n",
    "    return row['shape_counts'] == ground_truth_counter\n",
    "\n",
    "# Process for every model in two_shapes_df\n",
    "models = ['generate_text_llava_1.5', 'generate_text_llava_1.6',\n",
    "          'generate_text_qwen', 'generate_text_internvl', 'generate_text_llava-one',\n",
    "          'generate_text_llama-3.2', 'generate_text_gpt-4-turbo',\n",
    "          'generate_text_gpt-4o', 'generate_text_janus', 'generate_text_molmo',\n",
    "         'generate_text_math-puma', 'generate_text_math-llava', 'generate_text_g-llava']\n",
    "\n",
    "# Initialize results dictionary to hold correct counts per model\n",
    "results = {}\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\nProcessing model: {model}\")\n",
    "    \n",
    "    # Extract shapes for each model's generated text\n",
    "    two_shapes_df[f'{model}_shape_counts'] = two_shapes_df[model].apply(extract_shapes)\n",
    "    \n",
    "    # Compare extracted shapes with ground truth\n",
    "    two_shapes_df[f'{model}_correct_first_step'] = two_shapes_df.apply(\n",
    "        lambda row: compare_counts({\n",
    "            'ground_truth_shapes': row['ground_truth_shapes'],\n",
    "            'shape_counts': row[f'{model}_shape_counts']\n",
    "        }),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results[model] = two_shapes_df[[f'{model}_shape_counts', 'ground_truth_shapes', f'{model}_correct_first_step']]\n",
    "\n",
    "# Display results for each model\n",
    "for model, result_df in results.items():\n",
    "    print(f\"\\nResults for {model}:\")\n",
    "    print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75043caa-f5bd-4dcc-808b-942313141c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_shapes_df[\"generate_text_janus\"] = two_shapes_df[\"generate_text_janus\"].str.replace(\"\\n\", \" \").str.replace(\"    \", \"\")#[4]\n",
    "#For each shape, specify the number of sides it has:\n",
    "two_shapes_df[\"generate_text_janus\"] = two_shapes_df[\"generate_text_janus\"].str.replace(\"For each shape, specify the number of sides it has:\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caaf7da-eac4-4577-96a2-70ef9ba0f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### total number of sides:\n",
    "two_shapes_df[\"generate_text_llama-3.2\"] = two_shapes_df[\"generate_text_llama-3.2\"].str.replace(\"total number of sides:\", \" \")\n",
    "two_shapes_df[\"generate_text_llama-3.2\"] = two_shapes_df[\"generate_text_llama-3.2\"].str.replace(\"infinite\", \"10000\")\n",
    "two_shapes_df[\"generate_text_llama-3.2\"] = two_shapes_df[\"generate_text_llama-3.2\"].str.replace(\":\", \".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1745f-ea5d-4718-a801-e479e9d50008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Reference dictionaries\n",
    "shape_sides_reference = {\n",
    "    \"triangle\": 3, \"square\": 4, \"rectangle\": 4, \"circle\": 0,\n",
    "    \"pentagon\": 5, \"hexagon\": 6, \"ellipse\": 0, \"diamond\": 4, \"cube\": 12, \"heptagon\": 7, \"octagon\": 8\n",
    "}\n",
    "\n",
    "# Spelled-out numbers mapping\n",
    "number_mapping = {\n",
    "    \"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4,\n",
    "    \"five\": 5, \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9, \"ten\": 10\n",
    "}\n",
    "\n",
    "# Function to convert spelled-out numbers to integers\n",
    "def convert_to_number(word):\n",
    "    return number_mapping.get(word, None)  # Returns None if not in mapping\n",
    "\n",
    "# Function to extract and validate shapes and sides\n",
    "def extract_shapes_and_sides(text, ground_truth_shapes):\n",
    "    # Extract text between \"2.\" and \"3.\"\n",
    "    match = re.search(r\"2\\.(.*?)3\\.\", text, re.DOTALL)\n",
    "    if not match:\n",
    "        return {}, False  # No match found\n",
    "\n",
    "    shapes_text = match.group(1).lower()  # Extracted portion and lowercase\n",
    "\n",
    "    # Check if the text explicitly states \"no shapes\"\n",
    "    if \"no shapes\" in shapes_text:\n",
    "        return {}, len(ground_truth_shapes) == 0  # Correct if ground truth also has no shapes\n",
    "\n",
    "    # Regex: Match shape names (singular & plural), followed by the first number appearing after them\n",
    "    shape_sides_matches = re.findall(\n",
    "        r\"\\b(triangle|triangles|square|squares|rectangle|rectangles|circle|circles|pentagon|pentagons|hexagon|hexagons|ellipse|ellipses|diamond|diamonds|cube|cubes|heptagon|heptagons|octagon|octagons)\\b.*?(\\d+)\", \n",
    "        shapes_text\n",
    "    )\n",
    "\n",
    "\n",
    "    # Build shape dictionary and validate sides\n",
    "    shape_sides_dict = {}\n",
    "    correct = True\n",
    "\n",
    "    for shape, sides in shape_sides_matches:\n",
    "        # Convert plural to singular for consistency\n",
    "        shape = shape.rstrip(\"s\")\n",
    "\n",
    "        # Convert sides to an integer\n",
    "        sides = int(sides) if sides.isdigit() else None  # Ensure conversion\n",
    "\n",
    "        if shape in shape_sides_dict:\n",
    "            shape_sides_dict[shape] += sides\n",
    "        else:\n",
    "            shape_sides_dict[shape] = sides\n",
    "\n",
    "        # Validate sides\n",
    "        if shape in shape_sides_reference and sides != shape_sides_reference[shape]:\n",
    "            correct = False\n",
    "\n",
    "    return shape_sides_dict, correct\n",
    "\n",
    "\n",
    "# Process for every model in `two_shapes_df`\n",
    "for model in models:\n",
    "    print(f\"\\nProcessing model: {model}\")\n",
    "    \n",
    "    # Apply extraction and validation to each model's generated text\n",
    "    two_shapes_df[f'{model}_shape_sides_dict'], two_shapes_df[f'{model}_correct_second_step'] = zip(\n",
    "        *two_shapes_df.apply(\n",
    "            lambda row: extract_shapes_and_sides(row[model], row['ground_truth_shapes']), axis=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Display results for each model\n",
    "for model in models:\n",
    "    print(f\"\\nResults for {model}:\")\n",
    "    print(two_shapes_df[[model, f'{model}_shape_sides_dict', f'{model}_correct_second_step']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f7fc3-df1a-4561-868d-3898b4e1bb89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to extract the total number of sides from text\n",
    "number_mapping = {\n",
    "    \"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4,\n",
    "    \"five\": 5, \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9, \"ten\": 10\n",
    "}\n",
    "\n",
    "def extract_total_sides(text):\n",
    "    # Match numeric total sides\n",
    "    match = re.search(r\"The total number of sides is (\\d+)\\.?\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    # Match verbal total sides\n",
    "    match_verbal = re.search(r\"The total number of sides is (\\w+)\\.?\", text, re.IGNORECASE)\n",
    "    if match_verbal:\n",
    "        verbal_number = match_verbal.group(1).lower()\n",
    "        return number_mapping.get(verbal_number)  # Convert verbal number to integer if available\n",
    "\n",
    "    all_numbers = re.findall(r\"(\\d+|zero|one|two|three|four|five|six|seven|eight|nine|ten)\", text, re.IGNORECASE)\n",
    "    if all_numbers:\n",
    "        last_number = all_numbers[-1].lower()\n",
    "        return int(last_number) if last_number.isdigit() else number_mapping.get(last_number)\n",
    "\n",
    "    return None\n",
    "\n",
    "# Function to check if the addition is correct based on shape_sides_dict\n",
    "def is_math_correct(row, model):\n",
    "    shape_sides = row[f'{model}_shape_sides_dict']\n",
    "    extracted_sum = row[f'{model}_extracted_sum']\n",
    "    \n",
    "    # Handle empty or None shape sides dict\n",
    "    if not shape_sides or extracted_sum is None:\n",
    "        return False\n",
    "    \n",
    "    total_calculated = sum(value if value is not None else 0 for value in shape_sides.values())\n",
    "    return extracted_sum == total_calculated\n",
    "    \n",
    "\n",
    "# Process for every model in `two_shapes_df`\n",
    "for model in models:\n",
    "    print(f\"\\nProcessing model: {model}\")\n",
    "    \n",
    "    # Extract total number of sides for each model's generated text\n",
    "    two_shapes_df[f'{model}_extracted_sum'] = two_shapes_df[model].apply(extract_total_sides)\n",
    "    \n",
    "    # Compare extracted sum with total ground truth sides\n",
    "    two_shapes_df[f'{model}_correct_third_step'] = two_shapes_df[f'{model}_extracted_sum'] == two_shapes_df['total_ground_truth_sides']\n",
    "    \n",
    "    # Validate if the addition is correct based on shape_sides_dict\n",
    "    two_shapes_df[f'{model}_correct_third_based_second'] = two_shapes_df.apply(\n",
    "        lambda row: is_math_correct(row, model), axis=1\n",
    "    )\n",
    "\n",
    "# Display results for each model\n",
    "for model in models:\n",
    "    print(f\"\\nResults for {model}:\")\n",
    "    print(two_shapes_df[\n",
    "        [model, f'{model}_extracted_sum', 'total_ground_truth_sides', f'{model}_correct_third_step', f'{model}_correct_third_based_second']\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0f8843-0d8b-4d0c-8321-11ce0968f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define phrases to check (case insensitive)\n",
    "invalid_phrases = [\n",
    "    \"the image contains no shapes\",\n",
    "    \"i'm sorry\",\n",
    "    \"there is no image provided for analysis\"\n",
    "]\n",
    "\n",
    "# Convert invalid phrases to a single regex pattern (case insensitive)\n",
    "invalid_pattern = re.compile('|'.join(map(re.escape, invalid_phrases)), re.IGNORECASE)\n",
    "\n",
    "# Function to invalidate steps based on phrases\n",
    "def invalidate_steps(row, model):\n",
    "    if invalid_pattern.search(row[model]):\n",
    "        return {f'{model}_correct_first_step': False,\n",
    "                f'{model}_correct_second_step': False,\n",
    "                f'{model}_correct_third_step': False,\n",
    "                f'{model}_correct_third_based_second': False}\n",
    "    return {f'{model}_correct_first_step': row[f'{model}_correct_first_step'],\n",
    "            f'{model}_correct_second_step': row[f'{model}_correct_second_step'],\n",
    "            f'{model}_correct_third_step': row[f'{model}_correct_third_step'],\n",
    "            f'{model}_correct_third_based_second': row[f'{model}_correct_third_based_second']}\n",
    "\n",
    "# Apply the invalidation for each model\n",
    "for model in models:\n",
    "    updated_steps = two_shapes_df.apply(lambda row: invalidate_steps(row, model), axis=1)\n",
    "    updated_steps_df = pd.DataFrame(updated_steps.tolist())  # Convert to a DataFrame\n",
    "    two_shapes_df.update(updated_steps_df)  # Update the original DataFrame with invalidated steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d16333-29a2-4bff-bed3-ed5f08ccb675",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_phrases = [\n",
    "    \"the image contains no shapes\",\n",
    "    \"i'm sorry\",\n",
    "    \"there is no image provided for analysis\"\n",
    "]\n",
    "\n",
    "# Convert invalid phrases to a single regex pattern (case insensitive)\n",
    "invalid_pattern = re.compile('|'.join(map(re.escape, invalid_phrases)), re.IGNORECASE)\n",
    "\n",
    "# Filter rows containing any of the invalid phrases\n",
    "invalid_rows = two_shapes_df[two_shapes_df[models].apply(\n",
    "    lambda row: any(invalid_pattern.search(str(row[model])) for model in models), axis=1\n",
    ")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b2134-dd8b-48aa-a2cb-78cfe0a1af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows[\"generate_text_qwen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfddf370-7f75-4c35-a941-13101229286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_rows[\"background_color\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484ffe95-ad57-45ab-b210-a378a1b32865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_step_1_false_predictions(row, model):\n",
    "    generated_text = row[model]\n",
    "    ground_truth_shapes = row['ground_truth_shapes']\n",
    "    \n",
    "    # Ensure ground_truth_shapes is a list with two identical shapes\n",
    "    if len(ground_truth_shapes) == 2 and ground_truth_shapes[0] == ground_truth_shapes[1]:\n",
    "        shape = ground_truth_shapes[0]\n",
    "        # Regex pattern to detect \"two [color] [shape]s\"\n",
    "        pattern = rf\"two\\s+\\w+\\s+{shape}s\"\n",
    "        if re.search(pattern, generated_text, re.IGNORECASE):\n",
    "            return True  # Correct the prediction to True\n",
    "        # Additional check for \"two shapes: [color] [shape]s\"\n",
    "        pattern_extended = rf\"two\\s+shapes:\\s+\\w+\\s+{shape}s\"\n",
    "        if re.search(pattern_extended, generated_text, re.IGNORECASE):\n",
    "            return True  # Correct the prediction to True\n",
    "    return row[f'{model}_correct_first_step']  # Return the original correctness value\n",
    "\n",
    "\n",
    "# Apply the correction for each model\n",
    "for model in models:\n",
    "    two_shapes_df[f'{model}_correct_first_step'] = two_shapes_df.apply(\n",
    "        lambda row: correct_step_1_false_predictions(row, model), axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652cf7a4-7f2c-4673-b152-f824414cc770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Convert ground truth shapes to tuples for grouping\n",
    "two_shapes_df['ground_truth_shapes_tuple'] = two_shapes_df['ground_truth_shapes'].apply(tuple)\n",
    "\n",
    "# Define a function to calculate accuracy per shape combination\n",
    "def calculate_accuracy_per_combo(two_shapes_df, models):\n",
    "    combo_accuracies = {}\n",
    "    for model in models:\n",
    "        # Extract the shape count column and create a correctness column\n",
    "        model_shape_counts_col = f'{model}_shape_counts'\n",
    "        correctness_col = f'{model}_correct'\n",
    "        \n",
    "        two_shapes_df[correctness_col] = two_shapes_df.apply(\n",
    "            lambda row: row[model_shape_counts_col] == dict(Counter(row['ground_truth_shapes'])),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Group by ground truth shape combinations (as tuples) and calculate accuracy\n",
    "        accuracy_by_combo = (\n",
    "            two_shapes_df.groupby('ground_truth_shapes_tuple')[correctness_col]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "            .rename(columns={correctness_col: 'accuracy'})\n",
    "        )\n",
    "        \n",
    "        # Sort results by accuracy (ascending)\n",
    "        accuracy_by_combo = accuracy_by_combo.sort_values(by='ground_truth_shapes_tuple', ascending=True).reset_index(drop=True)\n",
    "        \n",
    "        # Store results for the model\n",
    "        combo_accuracies[model] = accuracy_by_combo\n",
    "\n",
    "    return combo_accuracies\n",
    "\n",
    "\n",
    "# Calculate accuracy per shape combination for each model\n",
    "combo_accuracies = calculate_accuracy_per_combo(two_shapes_df, models)\n",
    "\n",
    "# Display results\n",
    "for model, accuracy_df in combo_accuracies.items():\n",
    "    print(f\"\\nAccuracy per Shape Combination for {model}:\")\n",
    "    print(accuracy_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15b655-a3a0-404e-a3b4-9145cb8a3760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy for a given column\n",
    "def calculate_accuracy(column):\n",
    "    return (two_shapes_df[column].sum() / len(two_shapes_df[column])) * 100  # Proportion of True values as a percentage\n",
    "\n",
    "# Loop through models and calculate step-wise accuracies\n",
    "for model in models:\n",
    "    print(f\"\\nStep-wise Accuracies for {model}:\")\n",
    "    \n",
    "    # Accuracy for Step 1 (shape extraction)\n",
    "    step_1_accuracy = calculate_accuracy(f'{model}_correct_first_step')\n",
    "    \n",
    "    # Accuracy for Step 2 (shape-side mapping validation)\n",
    "    step_2_accuracy = calculate_accuracy(f'{model}_correct_second_step')\n",
    "    \n",
    "    # Accuracy for Step 3 (total side extraction validation)\n",
    "    step_3_accuracy = calculate_accuracy(f'{model}_correct_third_step')\n",
    "    \n",
    "    # Accuracy for Step 3 based on Step 2 (math correctness)\n",
    "    step_3_2_accuracy = calculate_accuracy(f'{model}_correct_third_based_second')\n",
    "    \n",
    "    # Display the results\n",
    "    print(f\"Step 1 Accuracy (correct_first_step): {step_1_accuracy:.2f}%\")\n",
    "    print(f\"Step 2 Accuracy (correct_second_step): {step_2_accuracy:.2f}%\")\n",
    "    print(f\"Step 3 Based on 2 Accuracy: {step_3_2_accuracy:.2f}%\")\n",
    "    print(f\"Step 3 Accuracy (correct_third_step): {step_3_accuracy:.2f}%\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11466947-c16e-472e-8177-871b64650132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f5956-e5a2-4a24-96b2-1f24fe31e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### abstract_shapes ###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad35e4-73b1-4882-b95e-1f822aabe759",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_combined_abstract_generated_texts.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000740ab-e8a6-44df-be7b-474ab0223f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all of these color: output/irregular_poly/shape_151_sides_7.png\n",
    "# Define the colors from the specific image\n",
    "bg_color_to_remove = df.loc[df[\"path\"] == \"output/irregular_poly/shape_151_sides_7.png\", \"background_color\"].values[0]\n",
    "shape_color_to_remove = df.loc[df[\"path\"] == \"output/irregular_poly/shape_151_sides_7.png\", \"shape_color\"].values[0]\n",
    "\n",
    "# Drop rows where both `background_color` and `shape_color` match\n",
    "df_filtered = df[~((df[\"background_color\"].astype(str) == bg_color_to_remove) & (df[\"shape_color\"].astype(str) == shape_color_to_remove))]\n",
    "\n",
    "# Display changes\n",
    "print(f\"Original DF Size: {len(df)}, New DF Size: {len(df_filtered)}\")\n",
    "\n",
    "# Update the dataframe\n",
    "df = df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a95ea-a4b9-4b26-970f-7297738bf2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"shape_type\"] = df[\"shape_type\"].fillna(\"irregular_polygon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9749308c-0b32-44dd-b2ba-deb3d36d2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"generate_text_llava-one\"] = df[\"generate_text_llava-one\"].str.split(\".assistant\\n\").str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e707018-6d85-4c9b-a667-16a4d2fa9130",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"generate_text_llama-3.2\"] = df[\"generate_text_llama-3.2\"].str.split(\".assistant\\n\\n\").str[-1].str.replace(\".\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85643d19-9b98-462c-bcca-14a1e9e8eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"generate_text_llama-3.2\"] = df[\"generate_text_llama-3.2\"].apply(lambda x: x.split(\"** \")[-1] if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a85a29-3080-44b2-8ac1-5292d226e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"generate_text_janus\"] = df[\"generate_text_janus\"].str.replace(\".\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc2ed6-10ad-4533-80a1-9a438c4186c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define a dictionary to map English numbers to digits\n",
    "num_translation = {\n",
    "    \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\", \"five\": \"5\",\n",
    "    \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\", \"ten\": \"10\",\n",
    "    \"eleven\": \"11\", \"twelve\": \"12\", \"thirteen\": \"13\", \"fourteen\": \"14\", \"fifteen\": \"15\",\n",
    "    \"sixteen\": \"16\", \"seventeen\": \"17\", \"eighteen\": \"18\", \"nineteen\": \"19\", \"twenty\": \"20\"\n",
    "}\n",
    "\n",
    "def clean_and_convert_numbers(text):\n",
    "    if isinstance(text, str):  # Ensure text is a string\n",
    "        text = text.lower().replace(\".\", \"\") #.replace(\" \", \"\")  # Remove periods and spaces\n",
    "        # Replace English numbers with digits using regex\n",
    "        for word, num in num_translation.items():\n",
    "            text = re.sub(rf\"\\b{word}\\b\", num, text)  # Ensure full word match\n",
    "    return text\n",
    "\n",
    "# Apply the function to both columns\n",
    "columns_to_clean = [ \"generate_text_llama-3.2\"]\n",
    "df[columns_to_clean] = df[columns_to_clean].applymap(clean_and_convert_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466bdca7-eea1-4f94-bf2f-72c7ba578acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# slightly different func\n",
    "def clean_and_convert_numbers(text):\n",
    "    if isinstance(text, str):  # Ensure text is a string\n",
    "        text = text.lower().replace(\".\", \"\") #.replace(\" \", \"\")  # Remove periods and spaces\n",
    "        # Replace English numbers with digits using regex\n",
    "        for word, num in num_translation.items():\n",
    "            text = re.sub(rf\"\\b{word}\\b\", num, text)  # Ensure full word match\n",
    "    return text\n",
    "\n",
    "# Apply the function to both columns\n",
    "columns_to_clean = [\"generate_text_janus\", \"generate_text_molmo\", \"generate_text_math-puma\"]\n",
    "df[columns_to_clean] = df[columns_to_clean].applymap(clean_and_convert_numbers)\n",
    "\n",
    "df[\"generate_text_janus\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8079449c-8a97-49ae-b866-e7f6d2768b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_last_number(text):\n",
    "    \"\"\"\n",
    "    Extracts the last number from a given text.\n",
    "    Returns None if no number is found.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return None  # Return None for non-string values\n",
    "    \n",
    "    numbers = re.findall(r'\\d+', text)  # Find all numbers in the text\n",
    "    return numbers[-1] if numbers else None  # Return the last number found\n",
    "\n",
    "# Apply function to the column\n",
    "df[\"generate_text_janus\"] = df[\"generate_text_janus\"].apply(extract_last_number)\n",
    "df[\"generate_text_janus\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede396c-e604-4740-9f60-af6876b44bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"generate_text_math-puma\"] = df[\"generate_text_math-puma\"].apply(extract_last_number)\n",
    "df[\"generate_text_math-puma\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc4aba-75e9-4e7f-b4d0-daae9756d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_columns = [\n",
    "    \"generate_text_llava_1.5\",\n",
    "    \"generate_text_llava_1.6\",\n",
    "    \"generate_text_qwen\",\n",
    "    \"generate_text_internvl\",\n",
    "    \"generate_text_llava-one\",\n",
    "    \"generate_text_llama-3.2\",\n",
    "    \"generate_text_gpt-4o\",\n",
    "    \"generate_text_gpt-4-turbo\",\n",
    "    \"generate_text_janus\", \n",
    "    \"generate_text_molmo\",\n",
    "    \"generate_text_g-llava\",\n",
    "    \"generate_text_math-llava\",\n",
    "    \"generate_text_math-puma\",\n",
    "]\n",
    "\n",
    "# Strip spaces only in model-generated text columns\n",
    "df[model_columns] = df[model_columns].applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "# Convert `num_sides` to string for comparison\n",
    "df[\"num_sides\"] = df[\"num_sides\"].astype(str).str.strip()\n",
    "\n",
    "# Compute accuracy for each model\n",
    "accuracy_results = {}\n",
    "for model in model_columns:\n",
    "    correct_predictions = (df[model].astype(str).str.strip() == df[\"num_sides\"]).sum()\n",
    "    accuracy = correct_predictions / len(df)  # Percentage accuracy\n",
    "    accuracy_results[model] = accuracy\n",
    "\n",
    "# Convert results to a DataFrame for visualization\n",
    "accuracy_df = pd.DataFrame(accuracy_results.items(), columns=[\"Model\", \"Accuracy\"])\n",
    "\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03132037-53fe-4ef4-a840-053bd38f4e98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df[model_columns] = df[model_columns].applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "# Convert `num_sides` to string for comparison\n",
    "df[\"num_sides\"] = df[\"num_sides\"].astype(str).str.strip()\n",
    "\n",
    "# Compute accuracy per shape_type\n",
    "accuracy_results = []\n",
    "for model in model_columns:\n",
    "    shape_accuracy = df.groupby(\"shape_type\").apply(\n",
    "        lambda group: (group[model].astype(str).str.strip() == group[\"num_sides\"]).mean()\n",
    "    ).reset_index()\n",
    "    shape_accuracy.columns = [\"shape_type\", model]\n",
    "    accuracy_results.append(shape_accuracy)\n",
    "\n",
    "# Merge accuracy results into a single DataFrame\n",
    "accuracy_df = accuracy_results[0]\n",
    "for acc_df in accuracy_results[1:]:\n",
    "    accuracy_df = accuracy_df.merge(acc_df, on=\"shape_type\")\n",
    "\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8451e4-407a-457f-899a-9505c4707cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc6075-c201-48d4-852a-ac8099109932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b616b11-bf15-4ff9-91b7-a631db7374d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## visual cues and cot ##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38aab20-bd2d-4721-941b-38e0885bded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_dfs = {\"heptagons_with_visual_cues\": None, \"arrow_on_plus_with_visual_cues\": None}  \n",
    "model_columns = {\n",
    "    \"llava-1.6\": \"generate_text_llava_1.6\",\n",
    "    \"llava-1.5\": \"generate_text_llava_1.5\",\n",
    "    \"qwen\": \"generate_text_qwen\",\n",
    "    \"internvl\": \"generate_text_internvl\",\n",
    "    \"llava-one\": \"generate_text_llava-one\",\n",
    "    \"llama-3.2\": \"generate_text_llama-3.2\",\n",
    "    \"molmo\": \"generate_text_molmo\",\n",
    "    \"janus\": \"generate_text_janus\",\n",
    "    \"gpt-4-turbo\": \"generate_text_gpt-4-turbo\",\n",
    "    \"gpt-4o\": \"generate_text_gpt-4o\",\n",
    "    \"math-llava\": \"generate_text_math-llava\",\n",
    "    \"g-llava\": \"generate_text_g-llava\",\n",
    "    \"math-puma\": \"generate_text_math-puma\",\n",
    "    \n",
    "    \n",
    "}\n",
    "additional_columns = {task: {col: None for col in model_columns.values()} for task in base_dfs.keys()}\n",
    "\n",
    "# Iterate over files in the current directory\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\".csv\"): #two_shapes_full.csv\n",
    "        for task in base_dfs.keys():  # Separate processing for 'two_shapes', 'shape_id', and 'traffic'\n",
    "            if task in file:\n",
    "                for model_version, column_name in model_columns.items():\n",
    "                    if f\"{model_version}_llava_\" in file:\n",
    "                        print(f\"Processing {file} for {task}...\")\n",
    "                        df = pd.read_csv(file)\n",
    "                        print(len(df))\n",
    "                        \n",
    "                        # For the first model (base_df), take all columns\n",
    "                        if base_dfs[task] is None and model_version == \"llava-1.5\":\n",
    "                            base_dfs[task] = df.copy()\n",
    "                            base_dfs[task].rename(columns={\"generated_text\": column_name}, inplace=True)\n",
    "                        else:\n",
    "                            # Just take the generated_text column and rename it\n",
    "                            additional_columns[task][column_name] = df[\"generated_text\"]\n",
    "\n",
    "# Append the additional columns to the base DataFrames\n",
    "for task, base_df in base_dfs.items():\n",
    "    if base_df is not None:\n",
    "        for column_name, column_data in additional_columns[task].items():\n",
    "            if column_data is not None:\n",
    "                base_df[column_name] = column_data\n",
    "\n",
    "        # Save the final combined DataFrame for each task\n",
    "        output_file = f\"final_combined_{task}_generated_texts.csv\"\n",
    "        base_df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved combined DataFrame for {task} to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44a987-5d60-4f93-b529-763e4681dbdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_combined_heptagons_with_visual_cues_generated_texts.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddece83-e265-401b-af0b-e211c717e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_number = {\n",
    "    \"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5,\n",
    "    \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9, \"ten\": 10\n",
    "}\n",
    "\n",
    "# List of columns to process\n",
    "columns_to_map = [\n",
    "    \"generate_text_llava_1.5\", \"generate_text_llava_1.6\", \"generate_text_llava-one\",\n",
    "    \"generate_text_qwen\", \"generate_text_internvl\",\n",
    "    \"generate_text_llama-3.2\", \"generate_text_gpt-4-turbo\",\n",
    "    \"generate_text_gpt-4o\",\"generate_text_molmo\",  \"generate_text_janus\",\n",
    "    \"generate_text_math-puma\",\"generate_text_math-llava\",  \"generate_text_g-llava\"\n",
    "]\n",
    "\n",
    "# Function to map words to numbers\n",
    "def map_words_to_numbers(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    mapped_numbers = [str(word_to_number.get(word.lower(), word)) for word in words]  # Map each word\n",
    "    return \" \".join(mapped_numbers)  # Recombine as a string\n",
    "\n",
    "# Apply the mapping to each column\n",
    "for col in columns_to_map:\n",
    "    df[col] = df[col].astype(str).apply(map_words_to_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eba5e9-d491-43e1-bea3-15a50599bcdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"generate_text_llava-one\"] = df[\"generate_text_llava-one\"].str.split(\"assistant\").str[-1]\n",
    "df[\"generate_text_llava-one\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e36ad1-d7a3-4f20-b8e9-03795dabcbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"generate_text_janus\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6f94a-8b7d-425d-b7b6-5b9af1812715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# List of columns to process\n",
    "columns_to_extract = [\n",
    "    \"generate_text_llava_1.5\", \"generate_text_llava_1.6\", \"generate_text_llava-one\",\n",
    "    \"generate_text_qwen\", \"generate_text_internvl\",\n",
    "    \"generate_text_llama-3.2\", \"generate_text_gpt-4-turbo\",\n",
    "    \"generate_text_gpt-4o\", \"generate_text_molmo\",  \"generate_text_janus\", \n",
    "     \"generate_text_math-puma\",\"generate_text_math-llava\",  \"generate_text_g-llava\"\n",
    "]\n",
    "\n",
    "# Function to extract the last number\n",
    "def extract_last_number(text):\n",
    "    # Find all numbers in the text\n",
    "    numbers = re.findall(r'\\d+', text)\n",
    "    \n",
    "    # Debugging: Print numbers found\n",
    "    print(f\"Extracted numbers: {numbers}\")\n",
    "\n",
    "    if numbers:\n",
    "        last_number = numbers[-1]\n",
    "\n",
    "        # Convert safely, checking for extremely large numbers\n",
    "        try:\n",
    "            last_number = int(last_number)\n",
    "\n",
    "            # If the number is too large, return None to avoid overflow\n",
    "            if last_number > np.iinfo(np.int64).max:\n",
    "                print(f\"Overflow detected: {last_number}\")\n",
    "                return None  \n",
    "\n",
    "            return last_number  # Return safe integer\n",
    "        except ValueError:\n",
    "            print(f\"Failed to convert: {last_number}\")\n",
    "            return None\n",
    "\n",
    "    return None  # Return None if no number is found\n",
    "\n",
    "\n",
    "# Apply the extraction to each column\n",
    "for col in columns_to_extract:\n",
    "    df[col] = df[col].astype(str).apply(extract_last_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea8a85-fb93-4347-9f05-5e9d689c7300",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_sides'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c502b-5228-4275-8333-b4fb2dfa5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "generate_text_columns = [col for col in df.columns if col.startswith(\"generate_text_\")]\n",
    "updated_columns = [col.replace(\"generate_text_\", \"\") for col in generate_text_columns]\n",
    "df = df.rename(columns=dict(zip(generate_text_columns, updated_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10274a-998b-4dd7-a6bd-a41f6cd746be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming `df` is your DataFrame with relevant columns\n",
    "generate_text_columns = [\n",
    "    \"generate_text_llava_1.5\", \"generate_text_llava_1.6\", \"generate_text_llava-one\",\n",
    "    \"generate_text_qwen\", \"generate_text_internvl\",\n",
    "    \"generate_text_llama-3.2\", \"generate_text_gpt-4-turbo\",\n",
    "    \"generate_text_gpt-4o\", \"generate_text_janus\", \"generate_text_molmo\",\n",
    "     \"generate_text_math-puma\",\"generate_text_math-llava\",  \"generate_text_g-llava\"\n",
    "]\n",
    "\n",
    "# Remove \"generate_text_\" prefix from column names\n",
    "updated_columns = [col.replace(\"generate_text_\", \"\") for col in generate_text_columns]\n",
    "\n",
    "# Rename the columns in the DataFrame\n",
    "df = df.rename(columns=dict(zip(generate_text_columns, updated_columns)))\n",
    "\n",
    "df = df[df[\"type\"].isin([\"ABC\", \"ABC_random\", \"123\",\"123_random\", \"plain\", \"CoT_123\", \"CoT_123_random\", \"CoT_ABC\", \"CoT_ABC_random\", \"CoT_plain\"])]\n",
    "#df = df[df[\"type\"].isin([\"ABC_random\", \"123_random\", \"plain\",  \"CoT_123_random\", \"CoT_ABC_random\", \"CoT_plain\"])]\n",
    "# Function to calculate accuracy for each column\n",
    "def calculate_accuracy_by_type(df, model_column, group_column, target_column):\n",
    "    df[\"correct\"] = df[model_column] == df[target_column]\n",
    "    accuracy_table = df.groupby(group_column)[\"correct\"].mean() * 100\n",
    "    return accuracy_table\n",
    "\n",
    "# Create a summary table for accuracies by model and type\n",
    "accuracy_tables = {}\n",
    "\n",
    "for model_column in updated_columns:\n",
    "    accuracy_tables[model_column] = calculate_accuracy_by_type(df, model_column, \"type\", \"num_sides\")\n",
    "\n",
    "# Combine accuracy tables into a single DataFrame\n",
    "accuracy_summary = pd.DataFrame(accuracy_tables)\n",
    "accuracy_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6900f9e6-ee61-4d54-aa22-cba1d497c310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2fc402-d56a-44ba-a402-53198b922a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### arrow_on_plus_with_visual_cues for VC-CoT ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf856bc-fec6-4797-9737-eb4f69f9acaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_combined_arrow_on_plus_with_visual_cues_generated_texts.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7276c3-98e0-4d9a-a73e-5d3f6f7c09dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_number = {\n",
    "    \"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5,\n",
    "    \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9, \"ten\": 10\n",
    "}\n",
    "\n",
    "# List of columns to process\n",
    "columns_to_map = [\n",
    "    \"generate_text_llava_1.5\", \"generate_text_llava_1.6\", \"generate_text_llava-one\",\n",
    "    \"generate_text_qwen\", \"generate_text_internvl\",\n",
    "    \"generate_text_llama-3.2\", \"generate_text_gpt-4-turbo\", \"generate_text_molmo\",\"generate_text_janus\",\n",
    "    \"generate_text_gpt-4o\",\n",
    "    \"generate_text_math-llava\",  \"generate_text_g-llava\",  \"generate_text_math-puma\"\n",
    "]\n",
    "\n",
    "# Function to map words to numbers\n",
    "def map_words_to_numbers(text):\n",
    "    words = text.split()  # Split the text into words\n",
    "    mapped_numbers = [str(word_to_number.get(word.lower(), word)) for word in words]  # Map each word\n",
    "    return \" \".join(mapped_numbers)  # Recombine as a string\n",
    "\n",
    "# Apply the mapping to each column\n",
    "for col in columns_to_map:\n",
    "    df[col] = df[col].astype(str).apply(map_words_to_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed61db6-c89d-40a7-a27d-73176765c68d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"generate_text_llava-one\"] = df[\"generate_text_llava-one\"].str.split(\"assistant\").str[-1]\n",
    "df[\"generate_text_llava-one\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e86c78-68f6-472e-9bff-23e012644bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# List of columns to process\n",
    "columns_to_extract = [\n",
    "    \"generate_text_llava_1.5\", \"generate_text_llava_1.6\", \"generate_text_llava-one\",\n",
    "    \"generate_text_qwen\", \"generate_text_internvl\", \"generate_text_molmo\",\"generate_text_janus\",\n",
    "    \"generate_text_llama-3.2\", \"generate_text_gpt-4-turbo\",\n",
    "    \"generate_text_gpt-4o\",  \"generate_text_math-llava\",\n",
    "    \"generate_text_math-llava\",  \"generate_text_g-llava\",  \"generate_text_math-puma\"# \"generate_text_gpt-o1\"\n",
    "]\n",
    "\n",
    "# Function to extract the last number\n",
    "def extract_last_number(text):\n",
    "    # Find all numbers in the text\n",
    "    numbers = re.findall(r'\\d+', text)\n",
    "    # Return the last number if found, otherwise None\n",
    "    return int(numbers[-1]) if numbers else None\n",
    "\n",
    "# Apply the extraction to each column\n",
    "for col in columns_to_extract:\n",
    "    df[col] = df[col].astype(str).apply(extract_last_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4bc50-84d7-460f-a34e-b34dd470ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_sides'] =15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ddbd44-5507-470f-bb2e-7752e819b0a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_text_columns = [col for col in df.columns if col.startswith(\"generate_text_\")]\n",
    "updated_columns = [col.replace(\"generate_text_\", \"\") for col in generate_text_columns]\n",
    "df = df.rename(columns=dict(zip(generate_text_columns, updated_columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c2cce-d2a5-4812-b6f9-ffeec35eca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming `df` is your DataFrame with relevant columns\n",
    "generate_text_columns = [\n",
    "    \"generate_text_llava_1.5\", \"generate_text_llava_1.6\", \"generate_text_llava-one\",\n",
    "    \"generate_text_qwen\", \"generate_text_internvl\", \"generate_text_molmo\",\"generate_text_janus\",\n",
    "    \"generate_text_llama-3.2\", \"generate_text_gpt-4-turbo\",\n",
    "    \"generate_text_gpt-4o\",  \n",
    "    \"generate_text_math-llava\",  \"generate_text_g-llava\",  \"generate_text_math-puma\"\n",
    "]\n",
    "\n",
    "# Remove \"generate_text_\" prefix from column names\n",
    "updated_columns = [col.replace(\"generate_text_\", \"\") for col in generate_text_columns]\n",
    "\n",
    "# Rename the columns in the DataFrame\n",
    "df = df.rename(columns=dict(zip(generate_text_columns, updated_columns)))\n",
    "\n",
    "df = df[df[\"type\"].isin([\"ABC\", \"ABC_random\", \"123\",\"123_random\", \"plain\", \"CoT_123\", \"CoT_123_random\", \"CoT_ABC\", \"CoT_ABC_random\", \"CoT_plain\"])]\n",
    "# Function to calculate accuracy for each column\n",
    "def calculate_accuracy_by_type(df, model_column, group_column, target_column):\n",
    "    df[\"correct\"] = df[model_column] == df[target_column]\n",
    "    accuracy_table = df.groupby(group_column)[\"correct\"].mean() * 100\n",
    "    return accuracy_table\n",
    "\n",
    "# Create a summary table for accuracies by model and type\n",
    "accuracy_tables = {}\n",
    "\n",
    "for model_column in updated_columns:\n",
    "    accuracy_tables[model_column] = calculate_accuracy_by_type(df, model_column, \"type\", \"num_sides\")\n",
    "\n",
    "# Combine accuracy tables into a single DataFrame\n",
    "accuracy_summary = pd.DataFrame(accuracy_tables)\n",
    "accuracy_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502aa0c-db39-4273-9256-dc0db770dc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75559c9-3651-4131-8613-5e418766e259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1108fe3c-50af-4f7d-94e3-32ecd7122ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### MathVerse for VC-CoT ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e84120-4976-4fe1-a2b9-4ea8c190a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_dfs = {\"mathverse_CoT\": None}  \n",
    "model_columns = {\n",
    "    \"llava-1.6\": \"generate_text_llava_1.6\",\n",
    "    \"llava-1.5\": \"generate_text_llava_1.5\",\n",
    "    \"qwen\": \"generate_text_qwen\",\n",
    "    \"internvl\": \"generate_text_internvl\",\n",
    "    \"llava-one\": \"generate_text_llava-one\",\n",
    "    \"llama-3.2\": \"generate_text_llama-3.2\",\n",
    "    \"molmo\": \"generate_text_molmo\",\n",
    "    \"janus\": \"generate_text_janus\",\n",
    "    \"gpt-4-turbo\": \"generate_text_gpt-4-turbo\",\n",
    "    \"gpt-4o\": \"generate_text_gpt-4o\",\n",
    "    \"math-llava\": \"generate_text_math-llava\",\n",
    "    \"g-llava\": \"generate_text_g-llava\",\n",
    "    \"math-puma\": \"generate_text_math-puma\"\n",
    "}\n",
    "\n",
    "\n",
    "additional_columns = {task: {col: None for col in model_columns.values()} for task in base_dfs.keys()}\n",
    "\n",
    "# Iterate over files in the current directory\n",
    "for file in os.listdir(\".\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        for task in base_dfs.keys():  # Separate processing for 'two_shapes', 'shape_id', and 'traffic'\n",
    "            if task in file:\n",
    "                for model_version, column_name in model_columns.items():\n",
    "                    if f\"{model_version}_llava_\" in file:\n",
    "                        print(f\"Processing {file} for {task}...\")\n",
    "                        df = pd.read_csv(file)\n",
    "                        print(len(df))\n",
    "                        \n",
    "                        # For the first model (base_df), take all columns\n",
    "                        if base_dfs[task] is None and model_version == \"llava-1.5\":\n",
    "                            base_dfs[task] = df.copy()\n",
    "                            base_dfs[task].rename(columns={\"generated_text\": column_name}, inplace=True)\n",
    "                        else:\n",
    "                            # Just take the generated_text column and rename it\n",
    "                            additional_columns[task][column_name] = df[\"generated_text\"]\n",
    "\n",
    "# Append the additional columns to the base DataFrames\n",
    "for task, base_df in base_dfs.items():\n",
    "    if base_df is not None:\n",
    "        for column_name, column_data in additional_columns[task].items():\n",
    "            if column_data is not None:\n",
    "                base_df[column_name] = column_data\n",
    "\n",
    "        # Save the final combined DataFrame for each task\n",
    "        output_file = f\"final_combined_{task}_generated_texts.csv\"\n",
    "        base_df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved combined DataFrame for {task} to '{output_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae0154f-7288-4614-a152-504002319053",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_combined_mathverse_CoT_generated_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f367ae-dedd-4c16-93d8-2b1d1cc3c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract columns that start with \"generate_text_\"\n",
    "model_columns = [col for col in df.columns if col.startswith(\"generate_text_\")]\n",
    "\n",
    "# Create a new dataframe to store last matches\n",
    "last_match_df = df.copy()\n",
    "\n",
    "# Apply regex extraction for each model column\n",
    "for col in model_columns:\n",
    "    last_match_df[f\"last_match_{col}\"] = df[col].apply(\n",
    "        lambda x: re.findall(r'[ABCDEF]', x)[-1] if isinstance(x, str) and re.findall(r'[ABCDEF]', x) else None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a0b57-0f69-4c27-be80-6731db8eb35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b0999f-de26-47d2-8f48-af1e30cbabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Extract model columns that start with \"last_match_generate_text_\"\n",
    "model_columns = [col for col in last_match_df.columns if col.startswith(\"last_match_generate_text_\")]\n",
    "\n",
    "# Drop rows with missing values in either any model column or 'answer'\n",
    "df_cleaned = last_match_df.dropna(subset=model_columns + ['answer'])\n",
    "\n",
    "# Convert columns to string type to avoid type issues\n",
    "df_cleaned[model_columns] = df_cleaned[model_columns].astype(str)\n",
    "df_cleaned['answer'] = df_cleaned['answer'].astype(str)\n",
    "\n",
    "# Initialize dictionary to store results per model\n",
    "model_metrics = {}\n",
    "\n",
    "# Compute metrics per model column\n",
    "for model_col in model_columns:\n",
    "    results = df_cleaned.groupby(\"type\").apply(\n",
    "        lambda group: pd.Series({\n",
    "            'F1-score': f1_score(group['answer'], group[model_col], average='macro') if not group.empty else None,\n",
    "            'Accuracy': accuracy_score(group['answer'], group[model_col]) if not group.empty else None\n",
    "        })\n",
    "    )\n",
    "    model_metrics[model_col] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a263a81-0865-49d9-b9cc-4d6473c62df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72ec79-6005-4542-b20e-288e409ce3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models = [\n",
    "    \"last_match_generate_text_molmo\",\n",
    "    \"last_match_generate_text_janus\",\n",
    "    \"last_match_generate_text_gpt-4o\",\n",
    "    \"last_match_generate_text_gpt-4-turbo\"\n",
    "]\n",
    "\n",
    "# Define the types to focus on\n",
    "selected_types = [\"mathverse_cot\", \"direct\", \"VC-CoT\"]\n",
    "\n",
    "# Extract the relevant accuracy data\n",
    "filtered_accuracy_results = {\n",
    "    model: data.loc[selected_types, \"Accuracy\"] for model, data in model_metrics.items() if model in selected_models\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "filtered_accuracy_df = pd.DataFrame(filtered_accuracy_results)\n",
    "\n",
    "filtered_accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d873f99-c15a-47b8-9285-328fe98bbaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
